{"cells":[{"cell_type":"markdown","id":"616125a5-44b0-4f3c-a80a-3f836432bb33","metadata":{"pycharm":{"name":"#%% md\n"},"tags":[],"id":"616125a5-44b0-4f3c-a80a-3f836432bb33"},"source":["$$\n","\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n","\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n","\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n","\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n","\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n","\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n","\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n","\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n","\\newcommand{\\set}[1]{\\mathbb {#1}}\n","\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n","\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n","\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n","$$\n","# Part 3: Binary Classification with Multilayer Perceptrons\n","<a id=part3></a>"]},{"cell_type":"markdown","id":"f0c9b321-1628-41c7-8dab-91437899c9ff","metadata":{"pycharm":{"name":"#%% md\n"},"id":"f0c9b321-1628-41c7-8dab-91437899c9ff"},"source":["In this part we'll implement a general purpose MLP and Binary Classifier using `pytorch`.\n","We'll implement its training, and also learn about decision boundaries an threshold selection in the context of binary classification. Finally, we'll explore the effect of depth and width on an MLP's performance."]},{"cell_type":"markdown","source":["### Porting to Google Colab\n","The following cell enables this notebook to run from Google Colab as well as from your local machine IDE.<br>\n","You can change `root_directory` and/or `this_notebook_google_path` to point to the directory in your Google account, which contains this notebook, together with the `hw2` sub-directory and its class files, the `imgs` sub-directory and the rest of the files.<br>"],"metadata":{"id":"aX4fmVYbM3__"},"id":"aX4fmVYbM3__"},{"cell_type":"code","source":["try:\n","    from google.colab import drive\n","    import sys\n","    import os\n","    root_directory = '/content/gdrive/'\n","    this_notebook_google_path = root_directory + 'Othercomputers/My Laptop/projects/RUNI/DL_TA/hw2'\n","    drive.mount(root_directory)\n","    # enable import python files from this notebook's path\n","    sys.path.append(this_notebook_google_path)\n","    # enable reading images and data files from this notebook's path\n","    os.chdir(this_notebook_google_path)\n","except:\n","    # no Google Colab --> fall back to local machine\n","    pass\n"],"metadata":{"id":"ZCUjZWNOM_HL"},"id":"ZCUjZWNOM_HL","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"9e441a96-79a3-4423-b5e4-23b54a4487ef","metadata":{"pycharm":{"name":"#%%\n"},"tags":[],"id":"9e441a96-79a3-4423-b5e4-23b54a4487ef"},"outputs":[],"source":["import os\n","import re\n","import sys\n","import glob\n","import unittest\n","from typing import Sequence, Tuple\n","\n","import sklearn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torchvision.transforms as tvtf\n","from torch import Tensor\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"id":"ca7a4eda-6598-46be-9c1c-df7487ab78e4","metadata":{"pycharm":{"name":"#%%\n"},"tags":[],"id":"ca7a4eda-6598-46be-9c1c-df7487ab78e4"},"outputs":[],"source":["seed = 42\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","plt.rcParams.update({'font.size': 12})\n","test = unittest.TestCase()"]},{"cell_type":"code","source":["def show_single_image(file_name: str) -> None:\n","    # Load the images\n","    image1 = mpimg.imread(file_name)\n","\n","    # Create subplots with 1 row and 1 columns\n","    fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n","\n","    # Plot the first image on the left subplot\n","    axes.imshow(image1)\n","    axes.axis('off')  # Turn off axis\n","    axes.set_title(file_name)  # Set title\n","\n","    # Adjust layout\n","    plt.tight_layout()\n","\n","    # Show the plot\n","    plt.show()\n"],"metadata":{"id":"UAdJyqmRM92F"},"id":"UAdJyqmRM92F","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"bf6e37bd-40a7-487b-b678-1962dd14e462","metadata":{"pycharm":{"name":"#%% md\n"},"tags":[],"id":"bf6e37bd-40a7-487b-b678-1962dd14e462"},"source":["## Synthetic Dataset"]},{"cell_type":"markdown","id":"9996a04e-8691-47c5-bfce-2a695a59d905","metadata":{"pycharm":{"name":"#%% md\n"},"id":"9996a04e-8691-47c5-bfce-2a695a59d905"},"source":["To test our first neural network-based classifiers we'll start by creating a toy binary classification dataset, but one which is not trivial for a linear model."]},{"cell_type":"code","execution_count":null,"id":"7a186f0f-af89-480f-af20-1ecdf6aa8e52","metadata":{"pycharm":{"name":"#%%\n"},"id":"7a186f0f-af89-480f-af20-1ecdf6aa8e52"},"outputs":[],"source":["from sklearn.datasets import make_moons\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"id":"dc91127c-211a-4b62-9e50-9032bf13334d","metadata":{"pycharm":{"name":"#%%\n"},"id":"dc91127c-211a-4b62-9e50-9032bf13334d"},"outputs":[],"source":["def rotate_2d(X, deg=0):\n","    \"\"\"\n","    Rotates each 2d sample in X of shape (N, 2) by deg degrees.\n","    \"\"\"\n","    a = np.deg2rad(deg)\n","    return X @ np.array([[np.cos(a), -np.sin(a)],[np.sin(a), np.cos(a)]]).T\n","\n","def plot_dataset_2d(X, y, n_classes=2, alpha=0.2, figsize=(8, 6), title=None, ax=None):\n","    if ax is None:\n","        fig, ax = plt.subplots(1, 1, figsize=figsize)\n","    for c in range(n_classes):\n","        ax.scatter(*X[y==c,:].T, alpha=alpha, label=f\"class {c}\");\n","\n","    ax.set_xlabel(\"$x_1$\"); ax.set_ylabel(\"$x_2$\");\n","    ax.legend(); ax.set_title((title or '') + f\" (n={len(y)})\")"]},{"cell_type":"markdown","id":"260b9d76-db7e-40be-a6ba-f14aa49cc5d2","metadata":{"pycharm":{"name":"#%% md\n"},"id":"260b9d76-db7e-40be-a6ba-f14aa49cc5d2"},"source":["We'll split our data into 80% train and validation, and 20% test.\n","To make it a bit more challenging, we'll simulate a somewhat real-world setting where there are multiple populations, and the training/validation data is not sampled iid from the underlying data distribution."]},{"cell_type":"code","execution_count":null,"id":"b054e214-f0e3-48e7-93a7-2d72f1961b36","metadata":{"pycharm":{"name":"#%%\n"},"tags":[],"id":"b054e214-f0e3-48e7-93a7-2d72f1961b36"},"outputs":[],"source":["np.random.seed(seed)\n","\n","N = 10_000\n","N_train = int(N * .8)\n","\n","# Create data from two different distributions for the training/validation\n","X1, y1 = make_moons(n_samples=N_train//2, noise=0.2)\n","X1 = rotate_2d(X1, deg=10)\n","X2, y2 = make_moons(n_samples=N_train//2, noise=0.25)\n","X2 = rotate_2d(X2, deg=50)\n","\n","# Test data comes from a similar but noisier distribution\n","X3, y3 = make_moons(n_samples=(N-N_train), noise=0.3)\n","X3 = rotate_2d(X3, deg=40)\n","\n","X, y = np.vstack([X1, X2, X3]), np.hstack([y1, y2, y3])"]},{"cell_type":"code","execution_count":null,"id":"83e2770d-5f64-4adf-b1bf-43967079dfae","metadata":{"pycharm":{"name":"#%%\n"},"id":"83e2770d-5f64-4adf-b1bf-43967079dfae"},"outputs":[],"source":["# Train and validation data is from mixture distribution\n","X_train, X_valid, y_train, y_valid = train_test_split(X[:N_train, :], y[:N_train], test_size=1/3, shuffle=False)\n","\n","# Test data is only from the second distribution\n","X_test, y_test = X[N_train:, :], y[N_train:]\n","\n","fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n","plot_dataset_2d(X_train, y_train, title='Train', ax=ax[0]);\n","plot_dataset_2d(X_valid, y_valid, title='Validation', ax=ax[1]);\n","plot_dataset_2d(X_test, y_test, title='Test', ax=ax[2]);"]},{"cell_type":"markdown","id":"f70852c5-365b-4ad3-a0a4-2f368f6c272d","metadata":{"pycharm":{"name":"#%% md\n"},"id":"f70852c5-365b-4ad3-a0a4-2f368f6c272d"},"source":["Now let us create a data loader for each dataset."]},{"cell_type":"code","execution_count":null,"id":"236c61d3-f89f-4a00-a8dc-0a7761edcb36","metadata":{"pycharm":{"name":"#%%\n"},"id":"236c61d3-f89f-4a00-a8dc-0a7761edcb36"},"outputs":[],"source":["from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","\n","batch_size = 32\n","\n","dl_train, dl_valid, dl_test = [\n","    DataLoader(\n","        dataset=TensorDataset(\n","            torch.from_numpy(X_).to(torch.float32),\n","            torch.from_numpy(y_)\n","        ),\n","        shuffle=True,\n","        num_workers=0,\n","        batch_size=batch_size\n","    )\n","    for X_, y_ in [(X_train, y_train), (X_valid, y_valid), (X_test, y_test)]\n","]\n","\n","print(f'{len(dl_train.dataset)=}, {len(dl_valid.dataset)=}, {len(dl_test.dataset)=}')"]},{"cell_type":"markdown","id":"cc2ed41b-56c9-4276-835f-ccf9418279f9","metadata":{"pycharm":{"name":"#%% md\n"},"tags":[],"id":"cc2ed41b-56c9-4276-835f-ccf9418279f9"},"source":["## Simple MLP"]},{"cell_type":"markdown","id":"15ab056b-bbb4-42e0-8da2-479ac769ce18","metadata":{"pycharm":{"name":"#%% md\n"},"id":"15ab056b-bbb4-42e0-8da2-479ac769ce18"},"source":["A multilayer-perceptron is arguably a the most basic type of neural network model.\n","It is composed of $L$ **layers**, each layer $l$ with $n_l$ **perceptron** (\"neuron\") units.\n","Each perceptron is connected to all ouputs of the previous layer (or all inputs in the first layer), calculates their weighted sum, applies a linearity and produces a single output."]},{"cell_type":"code","source":["show_single_image('imgs/mlp.png')"],"metadata":{"id":"TXXAUlF1Nb-N"},"id":"TXXAUlF1Nb-N","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"e9cd7e74-e4b4-45e6-995a-56aca6d2912e","metadata":{"pycharm":{"name":"#%% md\n"},"id":"e9cd7e74-e4b4-45e6-995a-56aca6d2912e"},"source":["Each layer $l$ operates on the output of the previous layer ($\\vec{y}_{l-1}$) and calculates:\n","\n","$$\n","\\vec{y}_l = \\varphi\\left( \\mat{W}_l \\vec{y}_{l-1} + \\vec{b}_l \\right),~\n","\\mat{W}_l\\in\\set{R}^{n_{l}\\times n_{l-1}},~ \\vec{b}_l\\in\\set{R}^{n_l},~ l \\in \\{1,2,\\dots,L\\}.\n","$$\n","\n","- Note that both input and output are **vectors**. We can think of the above equation as describing a layer of **multiple perceptrons**.\n","- We'll henceforth refer to such layers as **fully-connected** or FC layers.\n","- The first layer accepts the input of the model, i.e. $\\vec{y}_0=\\vec{x}\\in\\set{R}^d$.\n","- The last layer, $L$, is the output layer, so $y_L$ is the output of the model.\n","- The layers $1, 2, \\dots, L-1$ are called hidden layers."]},{"cell_type":"markdown","id":"a8ae72f8-4ccc-47bf-a29c-1a4d819077ab","metadata":{"pycharm":{"name":"#%% md\n"},"id":"a8ae72f8-4ccc-47bf-a29c-1a4d819077ab"},"source":["To begin, let's implement a general multi-layer perceptron model.\n","We'll seek to implement it in a way which is both general in terms of architecture, and also composable so that we can use our MLP in the context of larger models."]},{"cell_type":"markdown","id":"84d578f2-79eb-4368-86dd-73ee4a272514","metadata":{"pycharm":{"name":"#%% md\n"},"id":"84d578f2-79eb-4368-86dd-73ee4a272514"},"source":["**TODO**: Implement the `MLP` class in the `hw2/mlp.py` module."]},{"cell_type":"code","execution_count":null,"id":"0d70e5bf-5571-483d-9402-e06f9533fba5","metadata":{"pycharm":{"name":"#%%\n"},"id":"0d70e5bf-5571-483d-9402-e06f9533fba5"},"outputs":[],"source":["from hw2.mlp import MLP\n","\n","mlp = MLP(\n","    in_dim=2,\n","    dims=[8, 16, 32, 64],\n","    nonlins=['relu', 'tanh', nn.LeakyReLU(0.314), 'softmax']\n",")\n","mlp"]},{"cell_type":"markdown","id":"c957d6d5-b901-4ded-975b-180c158d0d8d","metadata":{"pycharm":{"name":"#%% md\n"},"id":"c957d6d5-b901-4ded-975b-180c158d0d8d"},"source":["Let's try our implementation on a batch of data."]},{"cell_type":"code","execution_count":null,"id":"2f23fa58-e592-4b94-bc6e-1880071a6689","metadata":{"pycharm":{"name":"#%%\n"},"id":"2f23fa58-e592-4b94-bc6e-1880071a6689"},"outputs":[],"source":["x0, y0 = next(iter(dl_train))\n","\n","yhat0 = mlp(x0)\n","\n","test.assertEqual(len([*mlp.parameters()]), 8)\n","test.assertEqual(yhat0.shape, (batch_size, mlp.out_dim))\n","test.assertTrue(torch.allclose(torch.sum(yhat0, dim=1), torch.tensor(1.0)))\n","test.assertIsNotNone(yhat0.grad_fn)\n","\n","yhat0"]},{"cell_type":"markdown","id":"fb897572-a13d-46a1-98eb-b19e1d6d7324","metadata":{"pycharm":{"name":"#%% md\n"},"tags":[],"id":"fb897572-a13d-46a1-98eb-b19e1d6d7324"},"source":["## MLP for Binary Classification"]},{"cell_type":"markdown","id":"beba74f1-71a2-4619-afba-919610c3a102","metadata":{"pycharm":{"name":"#%% md\n"},"id":"beba74f1-71a2-4619-afba-919610c3a102"},"source":["The MLP model we've implemented, while useful, is very general.\n","For the task of binary classification, we would like to add some additional functionality to it: the ability to output a normalized score for a sample being in class one (which we interpret as a probability) and a prediction based on some threshold of this probability.\n","In addition, we need some way to calculate a meaningful threshold based on the data and a trained model at hand.\n","\n","In order to maintain generality, we'll add this functionlity in the form of a wrapper: A `BinaryClassifier` class that can wrap any model producing two output features, and provide the the functionality stated above."]},{"cell_type":"markdown","id":"7c25cc39-cb26-4ca8-a914-6775b59eb540","metadata":{"pycharm":{"name":"#%% md\n"},"id":"7c25cc39-cb26-4ca8-a914-6775b59eb540"},"source":["**TODO**: In the `hw2/classifier.py` module, implement the `BinaryClassifier` and the missing parts of its base class, `Classifier`. Read the method documentation carefully and implement accordingly.\n","You can ignore the `roc_threshold` method at this stage."]},{"cell_type":"code","execution_count":null,"id":"c019ee88-09ad-4a2b-8ead-92bfe5908388","metadata":{"pycharm":{"name":"#%%\n"},"id":"c019ee88-09ad-4a2b-8ead-92bfe5908388"},"outputs":[],"source":["from hw2.classifier import BinaryClassifier\n","\n","bmlp4 = BinaryClassifier(\n","    model=MLP(in_dim=2, dims=[*[10]*3, 2], nonlins=[*['relu']*3, 'none']),\n","    threshold=0.5\n",")\n","print(bmlp4)\n","\n","# Test model\n","test.assertEqual(len([*bmlp4.parameters()]), 8)\n","test.assertIsNotNone(bmlp4(x0).grad_fn)\n","\n","# Test forward\n","yhat0_scores = bmlp4(x0)\n","test.assertEqual(yhat0_scores.shape, (batch_size, 2))\n","test.assertFalse(torch.allclose(torch.sum(yhat0_scores, dim=1), torch.tensor(1.0)))\n","\n","# Test predict_proba\n","yhat0_proba = bmlp4.predict_proba(x0)\n","test.assertEqual(yhat0_proba.shape, (batch_size, 2))\n","test.assertTrue(torch.allclose(torch.sum(yhat0_proba, dim=1), torch.tensor(1.0)))\n","\n","# Test classify\n","yhat0 = bmlp4.classify(x0)\n","test.assertEqual(yhat0.shape, (batch_size,))\n","test.assertEqual(yhat0.dtype, torch.int)\n","test.assertTrue(all(yh_ in (0, 1) for yh_ in yhat0))"]},{"cell_type":"markdown","id":"b3dbe90b-1be0-4f11-9ee4-39d9f2a2d15b","metadata":{"pycharm":{"name":"#%% md\n"},"id":"b3dbe90b-1be0-4f11-9ee4-39d9f2a2d15b"},"source":["### Training"]},{"cell_type":"markdown","id":"f83cb358-367e-49d9-a926-95f24a740440","metadata":{"pycharm":{"name":"#%% md\n"},"id":"f83cb358-367e-49d9-a926-95f24a740440"},"source":["Now that we have a classifier, we need to train it.\n","We will abstract the various aspects of training such as mlutiple epochs, iterating over batches, early stopping and saving model checkpoints, into a `Trainer` that will take care of these concerns."]},{"cell_type":"markdown","id":"083d2a94-78b1-40ed-ab8a-98801e6cf563","metadata":{"pycharm":{"name":"#%% md\n"},"id":"083d2a94-78b1-40ed-ab8a-98801e6cf563"},"source":["The `Trainer` class splits the task of training (and evaluating) models into three conceptual levels,\n","- Multiple epochs - the `fit` method, which returns a `FitResult` containing losses and accuracies for all epochs.\n","- Single epoch - the `train_epoch` and `test_epoch` methods, which return an `EpochResult` containing losses per batch and the single accuracy result of the epoch.\n","- Single batch - the `train_batch` and `test_batch` methods, which return a `BatchResult` containing a single loss and the number of correctly classified samples in the batch.\n","\n","It implements the first two levels. Inheriting classes are expected to implement the single-batch level methods since these are model and/or task specific."]},{"cell_type":"markdown","id":"681d4897-3a6b-4538-bf5f-79e4a14a9819","metadata":{"pycharm":{"name":"#%% md\n"},"id":"681d4897-3a6b-4538-bf5f-79e4a14a9819"},"source":["**TODO**:\n","\n","1. Implement the `Trainer`'s `fit` method and the `ClassifierTrainer`'s `train_batch`/`test_batch` methods, in the `hw2/training.py` module. You may ignore the Optional parts about early stopping an model checkpoints at this stage.\n","\n","2. Set the model's architecture hyper-parameters and the optimizer hyperparameters in `part3_arch_hp()` and `part3_optim_hp()`, respectively, in `hw2/answers.py`.\n","\n","Since this is a toy dataset, you should be able to quickly get above 85% accuracy even on the test set."]},{"cell_type":"code","execution_count":null,"id":"103848c9-81fb-4d28-85c5-8b3a0dd166c2","metadata":{"pycharm":{"name":"#%%\n"},"id":"103848c9-81fb-4d28-85c5-8b3a0dd166c2"},"outputs":[],"source":["from hw2.training import ClassifierTrainer\n","from hw2.answers import part3_arch_hp, part3_optim_hp\n","\n","torch.manual_seed(seed)\n","\n","hp_arch = part3_arch_hp()\n","hp_optim = part3_optim_hp()\n","\n","model = BinaryClassifier(\n","    model=MLP(\n","        in_dim=2,\n","        dims=[*[hp_arch['hidden_dims'],]*hp_arch['n_layers'], 2],\n","        nonlins=[*[hp_arch['activation'],]*hp_arch['n_layers'], hp_arch['out_activation']]\n","    ),\n","    threshold=0.5,\n",")\n","print(model)\n","\n","loss_fn = hp_optim.pop('loss_fn')\n","optimizer = torch.optim.SGD(params=model.parameters(), **hp_optim)\n","trainer = ClassifierTrainer(model, loss_fn, optimizer)\n","\n","fit_result = trainer.fit(dl_train, dl_valid, num_epochs=20, print_every=10);\n","\n","test.assertGreaterEqual(fit_result.train_acc[-1], 85.0)\n","test.assertGreaterEqual(fit_result.test_acc[-1], 75.0)"]},{"cell_type":"code","execution_count":null,"id":"528caaab-dd42-40d7-9db7-944726e7d35d","metadata":{"pycharm":{"name":"#%%\n"},"id":"528caaab-dd42-40d7-9db7-944726e7d35d"},"outputs":[],"source":["from cs236781.plot import plot_fit\n","\n","plot_fit(fit_result, log_loss=False, train_test_overlay=True);"]},{"cell_type":"markdown","id":"7a029b22-155b-46bd-b599-a5cfb3e979b0","metadata":{"pycharm":{"name":"#%% md\n"},"tags":[],"id":"7a029b22-155b-46bd-b599-a5cfb3e979b0"},"source":["### Decision Boundary"]},{"cell_type":"markdown","id":"59a5a191-7696-423d-9366-0d5d57fc8b8e","metadata":{"pycharm":{"name":"#%% md\n"},"id":"59a5a191-7696-423d-9366-0d5d57fc8b8e"},"source":["An important part of understanding what a non-linear classifier like our MLP is doing is visualizing it's decision boundaries. When we only have two input features, these are relatively simple to visualize, since we can simply plot our data on the plane, and evaluate our classifier on a constant 2D grid in order to approximate the decision boundary."]},{"cell_type":"markdown","id":"0aab8d8a-52a6-4844-9a11-3ac61ae74427","metadata":{"pycharm":{"name":"#%% md\n"},"id":"0aab8d8a-52a6-4844-9a11-3ac61ae74427"},"source":["**TODO**: Implement the `plot_decision_boundary_2d` function in the `hw2/classifier.py` module."]},{"cell_type":"code","execution_count":null,"id":"235a7e9b-23b6-4c67-ab1e-fcfcf1f588f3","metadata":{"pycharm":{"name":"#%%\n"},"id":"235a7e9b-23b6-4c67-ab1e-fcfcf1f588f3"},"outputs":[],"source":["from hw2.classifier import plot_decision_boundary_2d\n","\n","fig, ax = plot_decision_boundary_2d(model, *dl_valid.dataset.tensors)"]},{"cell_type":"markdown","id":"4d5eceab-8dd9-46bc-9ad6-eb2885b3dc11","metadata":{"pycharm":{"name":"#%% md\n"},"tags":[],"id":"4d5eceab-8dd9-46bc-9ad6-eb2885b3dc11"},"source":["### Threshold Selection"]},{"cell_type":"markdown","id":"4f90dbe0-87f4-49a2-a53e-5aa516055fbf","metadata":{"pycharm":{"name":"#%% md\n"},"id":"4f90dbe0-87f4-49a2-a53e-5aa516055fbf"},"source":["Another important component, especially in the context of binary classification is threshold selection. Until now, we arbitrarily chose a threshold of 0.5 when deciding the class label based on the probability score we calculated via softmax. In other words, we classified a sample to class 1 (the 'positive' class) when it's probability score was greater or equal to 0.5.\n","\n","However, in real-world classifiction problems we'll need to choose our threshold wisely based on the domain-specific requirements of the problem.\n","For example, depending on our application, we might care more about high sensitivity (correctly classifying positive examples), while for other applications specificity (correctly classifying negative examples) is more important."]},{"cell_type":"markdown","id":"fbcc2da7-4c4a-451c-8f85-3f82264b6deb","metadata":{"pycharm":{"name":"#%% md\n"},"id":"fbcc2da7-4c4a-451c-8f85-3f82264b6deb"},"source":["One way to understand the mistakes a model is making is to look at its [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix). From it, we easily see e.g. the false-negative rate (FNR) and false-positive rate (FPR).\n","\n","Let's look at the confusion matrices on the test and validation data using the model we trained above."]},{"cell_type":"code","execution_count":null,"id":"45be3635-b775-4a10-915e-4744c873f917","metadata":{"pycharm":{"name":"#%%\n"},"id":"45be3635-b775-4a10-915e-4744c873f917"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","def plot_confusion(classifier, x: np.ndarray, y: np.ndarray, ax=None):\n","    y_hat = classifier.classify(torch.from_numpy(x).to(torch.float32)).numpy()\n","    conf_mat = confusion_matrix(y, y_hat, normalize='all')\n","    ConfusionMatrixDisplay(conf_mat).plot(ax=ax, colorbar=False)\n","\n","model.threshold = 0.5\n","\n","_, axes = plt.subplots(1, 2, figsize=(10, 5))\n","axes[0].set_title(\"Train\"); axes[1].set_title(\"Validation\");\n","plot_confusion(model, X_train, y_train, ax=axes[0])\n","plot_confusion(model, X_valid, y_valid, ax=axes[1])"]},{"cell_type":"markdown","id":"570537e3-35fd-4d85-8499-3dbbb51300d5","metadata":{"pycharm":{"name":"#%% md\n"},"id":"570537e3-35fd-4d85-8499-3dbbb51300d5"},"source":["We can see that the model makes a different number of false-posiive and false-negative errors.\n","Clearly, this proportion would change if the classification threshold was different.\n","\n","A very common way to select the classification threshold is to find a threshold which optimally balances between the FPR and FNR.\n","This can be done by plotting the model's [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve, which shows 1-FNR vs. FPR for multiple threshold values, and selecting the point closest to the ideal point (`(0, 1)`)."]},{"cell_type":"markdown","id":"0275b3f8-63c6-4f91-b0b8-e6c189d30736","metadata":{"pycharm":{"name":"#%% md\n"},"id":"0275b3f8-63c6-4f91-b0b8-e6c189d30736"},"source":["**TODO**: Implement the `select_roc_thresh` function in the `hw2.classifier` module."]},{"cell_type":"code","execution_count":null,"id":"5b8e5e51-4ebf-4ec8-843a-f7a5288a5952","metadata":{"pycharm":{"name":"#%%\n"},"id":"5b8e5e51-4ebf-4ec8-843a-f7a5288a5952"},"outputs":[],"source":["from hw2.classifier import select_roc_thresh\n","\n","\n","optimal_thresh = select_roc_thresh(model, *dl_valid.dataset.tensors, plot=True)"]},{"cell_type":"markdown","id":"e9fbd531-00b4-4b89-8a38-8c8b46966d76","metadata":{"pycharm":{"name":"#%% md\n"},"id":"e9fbd531-00b4-4b89-8a38-8c8b46966d76"},"source":["Let's see the effect of our threshold selection on the confusion matrix and decision boundary."]},{"cell_type":"code","execution_count":null,"id":"b80a88eb-539c-4279-9666-83b2a80eae96","metadata":{"pycharm":{"name":"#%%\n"},"id":"b80a88eb-539c-4279-9666-83b2a80eae96"},"outputs":[],"source":["model.threshold = optimal_thresh\n","\n","_, axes = plt.subplots(1, 2, figsize=(10, 5))\n","axes[0].set_title(\"Train\"); axes[1].set_title(\"Validation\");\n","plot_confusion(model, X_train, y_train, ax=axes[0])\n","plot_confusion(model, X_valid, y_valid, ax=axes[1])\n","fig, ax = plot_decision_boundary_2d(model, *dl_valid.dataset.tensors)"]},{"cell_type":"markdown","id":"0556b168-a25e-46ae-b0fc-da829410e326","metadata":{"pycharm":{"name":"#%% md\n"},"tags":[],"id":"0556b168-a25e-46ae-b0fc-da829410e326"},"source":["### Architecture Experiments"]},{"cell_type":"markdown","id":"5003b586-a500-41a1-bb8a-995243eb5c62","metadata":{"pycharm":{"name":"#%% md\n"},"id":"5003b586-a500-41a1-bb8a-995243eb5c62"},"source":["Now, equipped with the tools we've implemented so far we'll expertiment with various MLP architectures.\n","We'll seek to study the effect of the models depth (number of hidden layers) and width (number of neurons per hidden layer) on the its decision boundaries and the resulting performance.\n","After training, we will use the validation set for threshold selection, and seek to maximize the performance on the test set."]},{"cell_type":"markdown","id":"4174a42c-3e19-486b-b1d2-874c3f7e82d4","metadata":{"pycharm":{"name":"#%% md\n"},"id":"4174a42c-3e19-486b-b1d2-874c3f7e82d4"},"source":["**TODO**: Implement the `mlp_experiment` function in `hw2/experiments.py`.\n","You are free to configure any model and optimization hyperparameters however you like, except for the specified `width` and `depth`.\n","Experiment with various options for these other hyperparameters and try to obtain the best results you can."]},{"cell_type":"code","execution_count":null,"id":"4572ad50-8ece-4e4d-ad76-7f0f3841757d","metadata":{"pycharm":{"name":"#%%\n"},"tags":[],"id":"4572ad50-8ece-4e4d-ad76-7f0f3841757d"},"outputs":[],"source":["from itertools import product\n","from tqdm.auto import tqdm\n","from hw2.experiments import mlp_experiment\n","\n","torch.manual_seed(seed)\n","\n","depths = [1, 2, 4]\n","widths = [2, 8, 32]\n","exp_configs = product(enumerate(widths), enumerate(depths))\n","fig, axes = plt.subplots(len(widths), len(depths), figsize=(10*len(depths), 10*len(widths)), squeeze=False)\n","test_accs = []\n","\n","for (i, width), (j, depth) in tqdm(list(exp_configs)):\n","    model, thresh, valid_acc, test_acc = mlp_experiment(\n","        depth, width, dl_train, dl_valid, dl_test, n_epochs=10\n","    )\n","    test_accs.append(test_acc)\n","    fig, ax = plot_decision_boundary_2d(model, *dl_test.dataset.tensors, ax=axes[i, j])\n","    ax.set_title(f\"{depth=}, {width=}\")\n","    ax.text(ax.get_xlim()[0]*.95, ax.get_ylim()[1]*.95, f\"{thresh=:.2f}\\n{valid_acc=:.1f}%\\n{test_acc=:.1f}%\", va=\"top\")\n","\n","# Assert minimal performance requirements.\n","# You should be able to do better than these by at least 5%.\n","test.assertGreaterEqual(np.min(test_accs), 75.0)\n","test.assertGreaterEqual(np.quantile(test_accs, 0.75), 85.0)"]},{"cell_type":"markdown","id":"8513cd7f-3e08-4191-9cd8-88c5824525a2","metadata":{"pycharm":{"name":"#%% md\n"},"tags":[],"id":"8513cd7f-3e08-4191-9cd8-88c5824525a2"},"source":["### Questions\n","\n","**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw2/answers.py`.\n"]},{"cell_type":"code","execution_count":null,"id":"acf923a0-fcce-4649-a2d8-48556992f69f","metadata":{"pycharm":{"name":"#%%\n"},"tags":[],"id":"acf923a0-fcce-4649-a2d8-48556992f69f"},"outputs":[],"source":["from cs236781.answers import display_answer\n","import hw2.answers"]},{"cell_type":"markdown","id":"1c943ac7-7d8d-4798-a491-ce17ef2a4f87","metadata":{"pycharm":{"name":"#%% md\n"},"tags":[],"id":"1c943ac7-7d8d-4798-a491-ce17ef2a4f87"},"source":["#### Question 1\n","\n","Consider the first binary classifier you trained in this notebook and the loss/accuracy curves we plotted for it on the train and validation sets, as well as the decision boundary plot.\n","\n","Based on those plots, explain **qualitatively** whether or now your model has:\n","1. High Optimization error?\n","2. High Generalization error?\n","2. High Approximation error?\n","\n","Explain your answers for each of the above.\n","Since this is a qualitative question, assume \"high\" simply means \"I would take measures in order to decrease it further\"."]},{"cell_type":"code","execution_count":null,"id":"0e2e5766-2fba-421c-9742-2a8f80aff969","metadata":{"pycharm":{"name":"#%%\n"},"id":"0e2e5766-2fba-421c-9742-2a8f80aff969"},"outputs":[],"source":["display_answer(hw2.answers.part3_q1)"]},{"cell_type":"markdown","id":"1cc9268b-0fce-4044-9ac3-fdc3787b8b71","metadata":{"pycharm":{"name":"#%% md\n"},"tags":[],"id":"1cc9268b-0fce-4044-9ac3-fdc3787b8b71"},"source":["#### Question 2\n","\n","Consider the first binary classifier you trained in this notebook and the confusion matrices we plotted for it.\n","\n","For the **validation** dataset, would you expect the FPR or the FNR to be higher, and why? Recall that you have full knowledge of the data generating process.\n"]},{"cell_type":"code","execution_count":null,"id":"d96df605-37e3-4f04-92e8-6b8fc03f145d","metadata":{"pycharm":{"name":"#%%\n"},"id":"d96df605-37e3-4f04-92e8-6b8fc03f145d"},"outputs":[],"source":["display_answer(hw2.answers.part3_q2)"]},{"cell_type":"markdown","id":"da41c22e-b6ce-48aa-b24c-32ee30330a9f","metadata":{"pycharm":{"name":"#%% md\n"},"tags":[],"id":"da41c22e-b6ce-48aa-b24c-32ee30330a9f"},"source":["#### Question 3\n","\n","You're training a binary classifier screening of a large cohort of patients for some disease, with the aim to detect the disease early, before any symptoms appear.\n","You train the model on easy-to-obtain features, so screening each individual patient is simple and low-cost.\n","In case the model classifies a patient as sick, she must then be sent to furhter testing in order to confirm the illness. Assume that these further tests are expensive and involve high-risk to the patient. Assume also that once diagnosed, a low-cost treatment exists.\n","\n","You wish to screen as many people as possible at the lowest possible cost and loss of life.\n","Would you still choose the same \"optimal\" point on the ROC curve as above?\n","If not, how would you choose it?\n","Answer these questions for two possible scenarios:\n","\n","1. A person with the disease will develop non-lethal symptoms that immediately confirm the diagnosis and can then be treated.\n","2. A person with the disease shows no clear symptoms and may die with high probability if not diagnosed early enough, either by your model or by the expensive test.\n","\n","Explain your answers."]},{"cell_type":"code","execution_count":null,"id":"8272378e-9b15-400b-ab19-da016fea6cda","metadata":{"pycharm":{"name":"#%%\n"},"id":"8272378e-9b15-400b-ab19-da016fea6cda"},"outputs":[],"source":["display_answer(hw2.answers.part3_q3)"]},{"cell_type":"markdown","id":"c5b9f702-7c78-4444-bf7b-45deef63f036","metadata":{"pycharm":{"name":"#%% md\n"},"tags":[],"id":"c5b9f702-7c78-4444-bf7b-45deef63f036"},"source":["#### Question 4\n","\n","Analyze your results from the Architecture Experiment.\n","\n","1. Explain the decision boundaries and model performance you obtained for the columns (fixed `depth`, `width` varies).\n","2. Explain the decision boundaries and model performance you obtained for the rows (fixed `width`, `depth` varies).\n","3. Compare and explain the results for the following pair of configurations, which have the same number of total parameters:\n","    - `depth=1, width=32` and `depth=4, width=8`\n","3. Explain the effect of threshold selection on the validation set: did it improve the results on the test set? why?"]},{"cell_type":"code","execution_count":null,"id":"420c1321-744d-427a-beed-817022bfa256","metadata":{"pycharm":{"name":"#%%\n"},"id":"420c1321-744d-427a-beed-817022bfa256"},"outputs":[],"source":["display_answer(hw2.answers.part3_q4)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}