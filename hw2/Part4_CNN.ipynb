{"cells":[{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"tags":[],"id":"1UMLsOTvN6dW"},"source":["$$\n","\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n","\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n","\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n","\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n","\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n","\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n","\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n","\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n","\\newcommand{\\set}[1]{\\mathbb {#1}}\n","\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n","\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n","\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n","$$\n","# Part 4: Convolutional Neural Networks\n","<a id=part4></a>"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"v-4OpqX0N6dY"},"source":["In this part we will explore convolution networks. We'll implement a common block-based deep CNN pattern with an without residual connections."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T21:38:20.378782Z","iopub.status.busy":"2022-04-28T21:38:20.378516Z","iopub.status.idle":"2022-04-28T21:38:21.882803Z","shell.execute_reply":"2022-04-28T21:38:21.882493Z"},"pycharm":{"name":"#%%\n"},"tags":[],"id":"Lp4ulwX_N6dZ"},"outputs":[],"source":["import os\n","import re\n","import sys\n","import glob\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import unittest\n","import torch\n","import torchvision\n","import torchvision.transforms as tvtf\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","source":["### Porting to Google Colab\n","The following cell enables this notebook to run from Google Colab as well as from your local machine IDE.<br>\n","You can change `root_directory` and/or `this_notebook_google_path` to point to the directory in your Google account, which contains this notebook, together with the `hw2` sub-directory and its class files, the `imgs` sub-directory and the rest of the files.<br>"],"metadata":{"id":"j-qd5RpCN_1F"}},{"cell_type":"code","source":["try:\n","    from google.colab import drive\n","    root_directory = '/content/gdrive/'\n","    this_notebook_google_path = root_directory + 'Othercomputers/My Laptop/projects/RUNI/DL_TA/hw2'\n","    drive.mount(root_directory)\n","    # enable import python files from this notebook's path\n","    sys.path.append(this_notebook_google_path)\n","    # enable reading images and data files from this notebook's path\n","    os.chdir(this_notebook_google_path)\n","except:\n","    # no Google Colab --> fall back to local machine\n","    pass\n"],"metadata":{"id":"yFVgzegGOC7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_single_image(file_name: str) -> None:\n","    # Load the images\n","    image1 = mpimg.imread(file_name)\n","\n","    # Create subplots with 1 row and 1 columns\n","    fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n","\n","    # Plot the first image on the left subplot\n","    axes.imshow(image1)\n","    axes.axis('off')  # Turn off axis\n","    axes.set_title(file_name)  # Set title\n","\n","    # Adjust layout\n","    plt.tight_layout()\n","\n","    # Show the plot\n","    plt.show()\n"],"metadata":{"id":"zXj7DACkORxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T21:38:21.885185Z","iopub.status.busy":"2022-04-28T21:38:21.885066Z","iopub.status.idle":"2022-04-28T21:38:21.901532Z","shell.execute_reply":"2022-04-28T21:38:21.901217Z"},"pycharm":{"name":"#%%\n"},"tags":[],"id":"tnp11BAfN6dZ"},"outputs":[],"source":["seed = 42\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","plt.rcParams.update({'font.size': 12})\n","test = unittest.TestCase()"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"ucYfBQ1sN6da"},"source":["## Reminder: Convolutional layers and networks\n","<a id=part3_1></a>"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"-rFHBWS2N6da"},"source":["Convolutional layers are the most essential building blocks of the state of the art deep learning image classification models and also play an important role in many other tasks.\n","As we saw in the tutorial, when applied to images, convolutional layers operate on and produce volumes (3D tensors) of activations."]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"ZQ5uPq49N6da"},"source":["A convenient way to interpret convolutional layers for images is as a collection of 3D learnable filters,\n","each of which operates on a small spatial region of the input volume.\n","Each filter is convolved with the input volume (\"slides over it\"),\n","and a dot product is computed at each location followed by a non-linearity which produces one activation.\n","All these activations produce a 2D plane known as a **feature map**.\n","Multiple feature maps (one for each filter) comprise the output volume."]},{"cell_type":"code","source":["show_single_image('imgs/cnn_filters.png')"],"metadata":{"id":"kFBPZyxuO0Pt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"Dcw-HfSGN6da"},"source":["A crucial property of convolutional layers is their translation equivariance, i.e. shifting the input results in\n","and equivalently shifted output.\n","This produces the ability to detect features regardless of their spatial location in the input."]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"rYd7R6jJN6da"},"source":["Convolutional network architectures usually follow a pattern basic repeating blocks: one or more convolution layers, each followed by a non-linearity (generally ReLU) and then a pooling layer to reduce spatial dimensions. Usually, the number of convolutional filters increases the deeper they are in the network.\n","These layers are meant to extract features from the input.\n","Then, one or more fully-connected layers is used to combine the extracted features into the required number of output class scores."]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"Af0GY3Z0N6da"},"source":["## Building convolutional networks with PyTorch\n","<a id=part3_2></a>"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"vHUySZz0N6da"},"source":["PyTorch provides all the basic building blocks needed for creating a convolutional arcitecture within the [`torch.nn`](https://pytorch.org/docs/stable/nn.html) package.\n","Let's use them to create a basic convolutional network with the following architecture pattern:\n","\n","    [(CONV -> ACT)*P -> POOL]*(N/P) -> (FC -> ACT)*M -> FC\n","\n","Here $N$ is the total number of convolutional layers,\n","$P$ specifies how many convolutions to perform before each pooling layer\n","and $M$ specifies the number of hidden fully-connected layers before the final output layer."]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"-m_Z0W7NN6da"},"source":["**TODO**: Complete the implementaion of the `CNN` class in the `hw2/cnn.py` module.\n","Use PyTorch's `nn.Conv2d` and `nn.MaxPool2d` for the convolution and pooling layers.\n","It's recommended to implement the missing functionality in the order of the class' methods."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T21:38:21.903703Z","iopub.status.busy":"2022-04-28T21:38:21.903596Z","iopub.status.idle":"2022-04-28T21:38:22.000758Z","shell.execute_reply":"2022-04-28T21:38:22.000357Z"},"pycharm":{"name":"#%%\n"},"tags":[],"id":"CXz90mPXN6da"},"outputs":[],"source":["from hw2.cnn import CNN\n","\n","test_params = [\n","    dict(\n","        in_size=(3,100,100), out_classes=10,\n","        channels=[32]*4, pool_every=2, hidden_dims=[100]*2,\n","        conv_params=dict(kernel_size=3, stride=1, padding=1),\n","        activation_type='relu', activation_params=dict(),\n","        pooling_type='max', pooling_params=dict(kernel_size=2),\n","    ),\n","    dict(\n","        in_size=(3,100,100), out_classes=10,\n","        channels=[32]*4, pool_every=2, hidden_dims=[100]*2,\n","        conv_params=dict(kernel_size=5, stride=2, padding=3),\n","        activation_type='lrelu', activation_params=dict(negative_slope=0.05),\n","        pooling_type='avg', pooling_params=dict(kernel_size=3),\n","    ),\n","    dict(\n","        in_size=(3,100,100), out_classes=3,\n","        channels=[16]*5, pool_every=3, hidden_dims=[100]*1,\n","        conv_params=dict(kernel_size=2, stride=2, padding=2),\n","        activation_type='lrelu', activation_params=dict(negative_slope=0.1),\n","        pooling_type='max', pooling_params=dict(kernel_size=2),\n","    ),\n","]\n","\n","for i, params in enumerate(test_params):\n","    torch.manual_seed(seed)\n","    net = CNN(**params)\n","    print(f\"\\n=== test {i=} ===\")\n","    print(net)\n","\n","    torch.manual_seed(seed)\n","    test_out = net(torch.ones(1, 3, 100, 100))\n","    print(f'{test_out=}')\n","\n","    expected_out = torch.load(f'tests/assets/expected_conv_out_{i:02d}.pt')\n","    print(f'max_diff={torch.max(torch.abs(test_out - expected_out)).item()}')\n","    test.assertTrue(torch.allclose(test_out, expected_out, atol=1e-3))"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"3ziTgr7PN6db"},"source":["As before, we'll wrap our model with a `Classifier` that provides the necessary functionality for calculating probability scores and obtaining class label predictions.\n","This time, we'll use a simple approach that simply selects the class with the highest score."]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"MxgujqxAN6db"},"source":["**TODO**: Implement the `ArgMaxClassifier` in the `hw2/classifier.py` module."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T21:38:22.003651Z","iopub.status.busy":"2022-04-28T21:38:22.003392Z","iopub.status.idle":"2022-04-28T21:38:22.281658Z","shell.execute_reply":"2022-04-28T21:38:22.281253Z"},"pycharm":{"name":"#%%\n"},"tags":[],"id":"ewxkyR3iN6db"},"outputs":[],"source":["from hw2.classifier import ArgMaxClassifier\n","\n","model = ArgMaxClassifier(model=CNN(**test_params[0]))\n","\n","test_image = torch.randint(low=0, high=256, size=(3, 100, 100), dtype=torch.float).unsqueeze(0)\n","test.assertEqual(model.classify(test_image).shape, (1,))\n","test.assertEqual(model.predict_proba(test_image).shape, (1, 10))\n","test.assertAlmostEqual(torch.sum(model.predict_proba(test_image)).item(), 1.0, delta=1e-3)"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"rZIBklxiN6db"},"source":["Let's now load CIFAR-10 to use as our dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T21:38:22.283880Z","iopub.status.busy":"2022-04-28T21:38:22.283744Z","iopub.status.idle":"2022-04-28T21:38:23.839965Z","shell.execute_reply":"2022-04-28T21:38:23.839646Z"},"pycharm":{"name":"#%%\n"},"tags":[],"id":"lFeJ68AaN6db"},"outputs":[],"source":["data_dir = os.path.expanduser('~/.pytorch-datasets')\n","ds_train = torchvision.datasets.CIFAR10(root=data_dir, download=True, train=True, transform=tvtf.ToTensor())\n","ds_test = torchvision.datasets.CIFAR10(root=data_dir, download=True, train=False, transform=tvtf.ToTensor())\n","\n","print(f'Train: {len(ds_train)} samples')\n","print(f'Test: {len(ds_test)} samples')\n","\n","x0,_ = ds_train[0]\n","in_size = x0.shape\n","num_classes = 10\n","print('input image size =', in_size)"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"Z1jihSLYN6dc"},"source":["Now as usual, as a sanity test let's make sure we can overfit a tiny dataset with our model. But first we need to adapt our `Trainer` for PyTorch models."]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"uOeEpEKGN6dc"},"source":["**TODO**:\n","1. Complete the implementaion of the `ClassifierTrainer` class in the `hw2/training.py` module if you haven't done so already.\n","2. Set the optimizer hyperparameters in `part4_optim_hp()`, respectively, in `hw2/answers.py`."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T21:38:23.841888Z","iopub.status.busy":"2022-04-28T21:38:23.841773Z","iopub.status.idle":"2022-04-28T21:38:28.567938Z","shell.execute_reply":"2022-04-28T21:38:28.567545Z"},"pycharm":{"name":"#%%\n"},"tags":[],"id":"yiHvpR9aN6dc"},"outputs":[],"source":["from hw2.training import ClassifierTrainer\n","from hw2.answers import part4_optim_hp\n","\n","torch.manual_seed(seed)\n","\n","# Define a tiny part of the CIFAR-10 dataset to overfit it\n","batch_size = 2\n","max_batches = 25\n","dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=False)\n","\n","# Create model, loss and optimizer instances\n","model = ArgMaxClassifier(\n","    model=CNN(\n","        in_size, num_classes, channels=[32], pool_every=1, hidden_dims=[100],\n","        conv_params=dict(kernel_size=3, stride=1, padding=1),\n","        pooling_params=dict(kernel_size=2),\n","    )\n",")\n","\n","hp_optim = part4_optim_hp()\n","loss_fn = hp_optim.pop('loss_fn')\n","optimizer = torch.optim.SGD(params=model.parameters(), **hp_optim)\n","\n","# Use ClassifierTrainer to run only the training loop a few times.\n","trainer = ClassifierTrainer(model, loss_fn, optimizer, device)\n","best_acc = 0\n","for i in range(25):\n","    res = trainer.train_epoch(dl_train, max_batches=max_batches, verbose=(i%5==0))\n","    best_acc = res.accuracy if res.accuracy > best_acc else best_acc\n","\n","# Test overfitting\n","test.assertGreaterEqual(best_acc, 90)"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"oe1HLUION6dc"},"source":["### Residual Networks"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"RvhtPma3N6dc"},"source":["A very common addition to the basic convolutional architecture described above are **shortcut connections**.\n","First proposed by [He et al. (2016)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf), this simple addition has been shown to be a crucial\n","ingredient in order to achieve effective learning with very deep networks.\n","Virtually all state of the art image classification models from recent years use this technique."]},{"cell_type":"markdown","source":["The idea is to add a shortcut, or skip, around every two or more convolutional layers:"],"metadata":{"id":"2WQqMejkPH-Y"}},{"cell_type":"code","source":["show_single_image('imgs/resnet_block2.png')"],"metadata":{"id":"yRXv8_0pPB2u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"kXOhBmybN6dc"},"source":["**On the left** we see an example of a regular Residual Block, that takes a 64 channel input, and performs two 3X3 convolutions , which are added to the original input.  \n","**On the right** we see an exapmle of a Bottleneck Residual Block, that takes a 256 channel input, projects it to a 64 channel tensor with a 1X1 convolution, then performs an inner 3X3 convolution, followd by another 1X1 projection convolution back to the original numer of channels, 256. The output is then added to the original input.\n","\n","Overall, we can denote the structure of the bottleneck channels in the given example as 256->64->64->256, where the first and last arrows denote the 1X1 convolutions, and the middle arrow is the inner convolution.\n","Note that the 1X1 convolution with the default parameters (in pytorch) is defined such that the only dimension of the tensor that changes is the number of channels.\n","\n","This adds an easy way for the network to learn identity mappings: set the weight values to be very small.\n","The outcome is that the convolutional layers learn a **residual** mapping, i.e. some delta that is applied\n","to the identity map, instead of actually learning a completely new mapping from scratch.\n"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"D34Los7_N6dc"},"source":["Lets start by implementing a general residual block, representing a structure similar to the above diagrams.\n","Our residual block will be composed of:\n","- A \"main path\" with some number of convolutional layers with ReLU between them. Optionally, we'll also apply dropout and  batch normalization layers (in this order) between the convolutions, before the ReLU.\n","- A \"shortcut path\" implementing an identity mapping around the main path. In case of a different number of input/output channels, the shortcut path should contain an additional `1x1` convolution to project the channel dimension.\n","- The sum of the main and shortcut paths output is passed though a ReLU and returned."]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"v6pJ2N4KN6dd"},"source":["**TODO**: Complete the implementation of the `ResidualBlock`'s `__init__()` method in the `hw2/cnn.py` module."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T21:38:28.570466Z","iopub.status.busy":"2022-04-28T21:38:28.570324Z","iopub.status.idle":"2022-04-28T21:38:28.605315Z","shell.execute_reply":"2022-04-28T21:38:28.605004Z"},"pycharm":{"name":"#%%\n"},"tags":[],"id":"9hQ9Rg1KN6dd"},"outputs":[],"source":["from hw2.cnn import ResidualBlock\n","\n","torch.manual_seed(seed)\n","\n","resblock = ResidualBlock(\n","    in_channels=3, channels=[6, 4]*2, kernel_sizes=[3, 5]*2,\n","    batchnorm=True, dropout=0.2\n",")\n","print(resblock)\n","\n","torch.manual_seed(seed)\n","test_out = resblock(torch.ones(1, 3, 32, 32))\n","print(f'{test_out.shape=}')\n","\n","expected_out = torch.load('tests/assets/expected_resblock_out.pt')\n","print(f'max_diff={torch.max(torch.abs(test_out - expected_out)).item()}')\n","test.assertTrue(torch.allclose(test_out, expected_out, atol=1e-3))"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"tags":[],"id":"eSuXpBd0N6dd"},"source":["#### Bottleneck Blocks"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"0r1wR4C8N6dd"},"source":["In the ResNet Block diagram shown above, the right block is called a bottleneck block.\n","This type of block is mainly used deep in the network, where the feature space becomes increasingly high-dimensional (i.e. there are many channels).\n","\n","Instead of applying a KxK conv layer on the original input channels, a bottleneck block\n","first projects to a lower number of features (channels), applies the KxK conv on the result, and then projects back to the original feature space.\n","Both projections are performed with 1x1 convolutions."]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"M9XVdZi9N6dd"},"source":["**TODO**: Complete the implementation of the `ResidualBottleneckBlock` in the `hw2/cnn.py` module."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T21:38:28.607405Z","iopub.status.busy":"2022-04-28T21:38:28.607293Z","iopub.status.idle":"2022-04-28T21:38:28.659720Z","shell.execute_reply":"2022-04-28T21:38:28.659411Z"},"pycharm":{"name":"#%%\n"},"tags":[],"id":"2MqSMBQ-N6dd"},"outputs":[],"source":["from hw2.cnn import ResidualBottleneckBlock\n","\n","torch.manual_seed(seed)\n","resblock_bn = ResidualBottleneckBlock(\n","    in_out_channels=256, inner_channels=[64, 32, 64], inner_kernel_sizes=[3, 5, 3],\n","    batchnorm=False, dropout=0.1, activation_type=\"lrelu\"\n",")\n","print(resblock_bn)\n","\n","# Test a forward pass\n","torch.manual_seed(seed)\n","test_in  = torch.ones(1, 256, 32, 32)\n","test_out = resblock_bn(test_in)\n","print(f'{test_out.shape=}')\n","assert test_out.shape == test_in.shape\n","\n","expected_out = torch.load('tests/assets/expected_resblock_bn_out.pt')\n","print(f'max_diff={torch.max(torch.abs(test_out - expected_out)).item()}')\n","test.assertTrue(torch.allclose(test_out, expected_out, atol=1e-3))"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"TlgN9NS-N6dd"},"source":["Now, based on the `ResidualBlock`, we'll implement our own variation of a residual network (ResNet),\n","with the following architecture:\n","\n","    [-> (CONV -> ACT)*P -> POOL]*(N/P) -> (FC -> ACT)*M -> FC\n","     \\------- SKIP ------/\n","     \n","Note that $N$, $P$ and $M$ are as before, however now $P$ also controls the number of convolutional layers to add a skip-connection to."]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"UDp1nuOfN6de"},"source":["**TODO**: Complete the implementation of the `ResNet` class in the `hw2/cnn.py` module.\n","You must use your `ResidualBlock`s or `ResidualBottleneckBlock`s to group together every $P$ convolutional layers."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T21:38:28.661659Z","iopub.status.busy":"2022-04-28T21:38:28.661551Z","iopub.status.idle":"2022-04-28T21:38:28.889340Z","shell.execute_reply":"2022-04-28T21:38:28.889032Z"},"pycharm":{"name":"#%%\n"},"tags":[],"id":"g8NUSsOoN6de"},"outputs":[],"source":["from hw2.cnn import ResNet\n","\n","test_params = [\n","    dict(\n","        in_size=(3,100,100), out_classes=10, channels=[32, 64]*3,\n","        pool_every=4, hidden_dims=[100]*2,\n","        activation_type='lrelu', activation_params=dict(negative_slope=0.01),\n","        pooling_type='avg', pooling_params=dict(kernel_size=2),\n","        batchnorm=True, dropout=0.1,\n","        bottleneck=False\n","    ),\n","    dict(\n","        # create 64->16->64 bottlenecks\n","        in_size=(3,100,100), out_classes=5, channels=[64, 16, 64]*4,\n","        pool_every=3, hidden_dims=[64]*1,\n","        activation_type='tanh',\n","        pooling_type='max', pooling_params=dict(kernel_size=2),\n","        batchnorm=True, dropout=0.1,\n","        bottleneck=True\n","    )\n","]\n","\n","for i, params in enumerate(test_params):\n","    torch.manual_seed(seed)\n","    net = ResNet(**params)\n","    print(f\"\\n=== test {i=} ===\")\n","    print(net)\n","\n","    torch.manual_seed(seed)\n","    test_out = net(torch.ones(1, 3, 100, 100))\n","    print(f'{test_out=}')\n","\n","    expected_out = torch.load(f'tests/assets/expected_resnet_out_{i:02d}.pt')\n","    print(f'max_diff={torch.max(torch.abs(test_out - expected_out)).item()}')\n","    test.assertTrue(torch.allclose(test_out, expected_out, atol=1e-3))"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"W7moiEOiN6de"},"source":["## Questions\n","<a id=part3_4></a>"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"FB8-BLqiN6de"},"source":["**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw2/answers.py`."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T21:38:28.891246Z","iopub.status.busy":"2022-04-28T21:38:28.891135Z","iopub.status.idle":"2022-04-28T21:38:28.912932Z","shell.execute_reply":"2022-04-28T21:38:28.912626Z"},"pycharm":{"name":"#%%\n"},"id":"8zu6_6bZN6de"},"outputs":[],"source":["from cs236781.answers import display_answer\n","import hw2.answers"]},{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%% md\n"},"id":"h5wqpkaeN6de"},"source":["### Question 1\n","\n","Consider the bottleneck block from the right side of the ResNet diagram above.\n","Compare it to a regular block that performs a two 3x3 convs directly on the 256-channel input (i.e. as shown in the left side of the diagram, with a different number of channels).\n","Explain the differences between the regular block and the bottleneck block in terms of:\n","\n","1. Number of parameters. Calculate the exact numbers for these two examples.\n","2. Number of floating point operations required to compute an output (qualitative assessment).\n","3. Ability to combine the input: (1) spatially (within feature maps); (2) across feature maps.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T21:38:28.914847Z","iopub.status.busy":"2022-04-28T21:38:28.914736Z","iopub.status.idle":"2022-04-28T21:38:28.934927Z","shell.execute_reply":"2022-04-28T21:38:28.934619Z"},"pycharm":{"name":"#%%\n"},"tags":[],"id":"NEs3s_noN6de"},"outputs":[],"source":["display_answer(hw2.answers.part4_q1)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}